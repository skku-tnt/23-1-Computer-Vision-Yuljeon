{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Resnet18, 34 구현\n",
        "참조: https://youtu.be/671BsKl8d0E.   \n",
        "https://github.com/ndb796/Deep-Learning-Paper-Review-and-Practice/blob/master/code_practices/ResNet18_CIFAR10_Train.ipynb"
      ],
      "metadata": {
        "id": "QGF8Ov6wc5hI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import Tensor\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.optim as optim\n",
        "import os\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms"
      ],
      "metadata": {
        "id": "2x0gIwXnQOyA"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 모델 쌓기"
      ],
      "metadata": {
        "id": "Y93E60g9dIxO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "sPUSbHC0iR2Y"
      },
      "outputs": [],
      "source": [
        "class BasicBlock(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels, stride = 1):\n",
        "    super(BasicBlock, self).__init__()\n",
        "\n",
        "    # 첫 번째 convolution layer\n",
        "    # input과 output의 차원 다를 때: stride로 너비, 높이 조절\n",
        "    self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3,\n",
        "                           stride=stride, padding=1, bias=False)\n",
        "    self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "    # 두 번째 convolution layer\n",
        "    self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3,\n",
        "                           stride=1, padding=1, bias=False)\n",
        "    self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "    # shortcut connections\n",
        "    # nn.Sequential(): 모듈들을 인수로 받아 순서대로 정렬해 둔 뒤 입력값이 들어오면 순서대로 모듈 실행하여 결과값 Return \n",
        "    self.shortcut = nn.Sequential()\n",
        "    if stride != 1: #stride가 1이 아님. 차원이 달라 identity mapping을 수행하지 않는 경우\n",
        "      self.shortcut = nn.Sequential(\n",
        "          nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
        "          nn.BatchNorm2d(out_channels)\n",
        "      )\n",
        "\n",
        "  def forward(self, x):\n",
        "    # F.relu: pytorch의 내장 relu 함수\n",
        "    # convolution layer 1 -> bn -> relu -> convolution layer 2 -> bn -> shortcut -> relu\n",
        "    out = F.relu(self.bn1(self.conv1(x)))\n",
        "    out = self.bn2(self.conv2(out))\n",
        "    out = out + self.shortcut(x)\n",
        "    out = F.relu(out)\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ResNet(nn.Module):\n",
        "  def __init__(self, block, num_blocks, num_classes=10):\n",
        "    super(ResNet, self).__init__()\n",
        "    self.in_channels = 64\n",
        "\n",
        "    # 3x3 필터 64개\n",
        "    # 하나의 convolution layer로 dimension 바꿔 줌\n",
        "    self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "    self.bn1 = nn.BatchNorm2d(64)\n",
        "\n",
        "    # basic block 쌓기\n",
        "    self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "    self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "    self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "    self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "    self.linear = nn.Linear(512, num_classes)\n",
        "\n",
        "  def _make_layer(self, block, out_channels, num_blocks, stride):\n",
        "    # strides: 너비, 높이 조절할 수 있는 1이 아닌 stride 값 + block 개수 만큼의 1\n",
        "    strides = [stride] + [1] * (num_blocks - 1)\n",
        "    layers = []\n",
        "    for stride in strides:\n",
        "      layers.append(block(self.in_channels, out_channels, stride))\n",
        "      self.in_channels = out_channels # 다음 레이어의 input 채널 수 변경\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = F.relu(self.bn1(self.conv1(x)))\n",
        "    out = self.layer1(out)\n",
        "    out = self.layer2(out)\n",
        "    out = self.layer3(out)\n",
        "    out = self.layer4(out)\n",
        "    out = F.avg_pool2d(out, 4)\n",
        "    # FC\n",
        "    out = out.view(out.size(0), -1)\n",
        "    out = self.linear(out)\n",
        "    return out"
      ],
      "metadata": {
        "id": "84YKh0AhRPHf"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Model:\n",
        "  def ResNet18():\n",
        "    return ResNet(BasicBlock, [2, 2, 2, 2])\n",
        "\n",
        "  def ResNet34():\n",
        "    return ResNet(BasicBlock, [3, 4, 6, 3])"
      ],
      "metadata": {
        "id": "qtpR3FiLWoV9"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 데이터셋 다운로드"
      ],
      "metadata": {
        "id": "dPOWVorTdOik"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(), # data augmentation \n",
        "    transforms.ToTensor(),])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),])\n",
        "\n",
        "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
        "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=4)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=100, shuffle=False, num_workers=4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uUPZcoiGY5pp",
        "outputId": "0030837e-bbce-4868-aa22-4f5daeec870f"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 학습 환경설정"
      ],
      "metadata": {
        "id": "Ti2-uTl7dRW2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda'\n",
        "\n",
        "net = Model.ResNet18()\n",
        "net = net.to(device)\n",
        "net = torch.nn.DataParallel(net)\n",
        "cudnn.benchmark = True\n",
        "\n",
        "learning_rate = 0.1 #0.1로 시작해서 점차 줄여나갈 예정\n",
        "file_name = 'resnet18_cifar10.pt'\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9, weight_decay=0.0001)"
      ],
      "metadata": {
        "id": "MAFCvI3uZB7g"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 학습 과정 설계"
      ],
      "metadata": {
        "id": "opnYQ8yIdUOM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def adjust_lr(optimizer, epoch):\n",
        "    lr = learning_rate\n",
        "    if epoch >= 100:\n",
        "        lr /= 10\n",
        "    if epoch >= 150:\n",
        "        lr /= 10\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "\n",
        "def train(epoch):\n",
        "    print('\\n[ Train %d]' % epoch)\n",
        "    net.train()\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        benign_outputs = net(inputs)\n",
        "        loss = criterion(benign_outputs, targets)\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = benign_outputs.max(1)\n",
        "\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "        \n",
        "        if batch_idx % 100 == 0:\n",
        "            print('\\nCurrent batch:', str(batch_idx))\n",
        "            print('Current benign train accuracy:', str(predicted.eq(targets).sum().item() / targets.size(0)))\n",
        "            print('Current benign train loss:', loss.item())\n",
        "\n",
        "    print('\\nTotal benign train accuarcy:', 100. * correct / total)\n",
        "    print('Total benign train loss:', train_loss)\n",
        "\n",
        "def test(epoch):\n",
        "    print('\\n[ Test %d]' % epoch)\n",
        "    net.eval()\n",
        "    loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for batch_idx, (inputs, targets) in enumerate(test_loader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        total += targets.size(0)\n",
        "\n",
        "        outputs = net(inputs)\n",
        "        loss += criterion(outputs, targets).item()\n",
        "\n",
        "        _, predicted = outputs.max(1)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "    print('\\nTest accuarcy:', 100. * correct / total)\n",
        "    print('Test average loss:', loss / total)\n",
        "\n",
        "    state = {\n",
        "        'net': net.state_dict()\n",
        "    }\n",
        "    if not os.path.isdir('checkpoint'):\n",
        "        os.mkdir('checkpoint')\n",
        "    torch.save(state, './checkpoint/' + file_name)\n",
        "    print('Model Saved!')"
      ],
      "metadata": {
        "id": "Ww5iq0hMZMDN"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 학습"
      ],
      "metadata": {
        "id": "hywZ7G5xddXH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(0, 20):\n",
        "    adjust_lr(optimizer, epoch)\n",
        "    train(epoch)\n",
        "    test(epoch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cALBvsUZaJKR",
        "outputId": "b8749ba5-5486-4b82-8e55-6cfa90aca282"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[ Train 0]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.0859375\n",
            "Current benign train loss: 2.4415760040283203\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.1640625\n",
            "Current benign train loss: 2.099649667739868\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.25\n",
            "Current benign train loss: 1.956758737564087\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.3125\n",
            "Current benign train loss: 1.8152332305908203\n",
            "\n",
            "Total benign train accuarcy: 26.366\n",
            "Total benign train loss: 811.7784942388535\n",
            "\n",
            "[ Test 0]\n",
            "\n",
            "Test accuarcy: 31.8\n",
            "Test average loss: 0.01968365511894226\n",
            "Model Saved!\n",
            "\n",
            "[ Train 1]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.390625\n",
            "Current benign train loss: 1.585494041442871\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.421875\n",
            "Current benign train loss: 1.6850682497024536\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.4609375\n",
            "Current benign train loss: 1.4859141111373901\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.53125\n",
            "Current benign train loss: 1.2798535823822021\n",
            "\n",
            "Total benign train accuarcy: 42.48\n",
            "Total benign train loss: 606.6087111234665\n",
            "\n",
            "[ Test 1]\n",
            "\n",
            "Test accuarcy: 42.94\n",
            "Test average loss: 0.016095615577697755\n",
            "Model Saved!\n",
            "\n",
            "[ Train 2]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.5234375\n",
            "Current benign train loss: 1.277796745300293\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.5625\n",
            "Current benign train loss: 1.2780290842056274\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.546875\n",
            "Current benign train loss: 1.1739256381988525\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.484375\n",
            "Current benign train loss: 1.3638484477996826\n",
            "\n",
            "Total benign train accuarcy: 52.586\n",
            "Total benign train loss: 508.00141763687134\n",
            "\n",
            "[ Test 2]\n",
            "\n",
            "Test accuarcy: 57.75\n",
            "Test average loss: 0.011810554122924805\n",
            "Model Saved!\n",
            "\n",
            "[ Train 3]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.625\n",
            "Current benign train loss: 1.047741174697876\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.6484375\n",
            "Current benign train loss: 1.1256580352783203\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.46875\n",
            "Current benign train loss: 1.511910080909729\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.5859375\n",
            "Current benign train loss: 1.1815385818481445\n",
            "\n",
            "Total benign train accuarcy: 60.494\n",
            "Total benign train loss: 431.4016405940056\n",
            "\n",
            "[ Test 3]\n",
            "\n",
            "Test accuarcy: 59.6\n",
            "Test average loss: 0.011832944190502168\n",
            "Model Saved!\n",
            "\n",
            "[ Train 4]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.59375\n",
            "Current benign train loss: 1.126578450202942\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.6796875\n",
            "Current benign train loss: 1.046489953994751\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.5703125\n",
            "Current benign train loss: 1.0829992294311523\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.8816165924072266\n",
            "\n",
            "Total benign train accuarcy: 66.3\n",
            "Total benign train loss: 372.89582842588425\n",
            "\n",
            "[ Test 4]\n",
            "\n",
            "Test accuarcy: 59.83\n",
            "Test average loss: 0.01273721376657486\n",
            "Model Saved!\n",
            "\n",
            "[ Train 5]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.7109375\n",
            "Current benign train loss: 0.9207994937896729\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.671875\n",
            "Current benign train loss: 0.8728364109992981\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.6875\n",
            "Current benign train loss: 0.8224097490310669\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.734375\n",
            "Current benign train loss: 0.7057616114616394\n",
            "\n",
            "Total benign train accuarcy: 70.542\n",
            "Total benign train loss: 327.47315549850464\n",
            "\n",
            "[ Test 5]\n",
            "\n",
            "Test accuarcy: 72.11\n",
            "Test average loss: 0.008072249644994737\n",
            "Model Saved!\n",
            "\n",
            "[ Train 6]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.6796875\n",
            "Current benign train loss: 0.8194884657859802\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.7265625\n",
            "Current benign train loss: 0.7840791344642639\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.671875\n",
            "Current benign train loss: 0.7992662191390991\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.7890625\n",
            "Current benign train loss: 0.6971753239631653\n",
            "\n",
            "Total benign train accuarcy: 74.364\n",
            "Total benign train loss: 285.9215087592602\n",
            "\n",
            "[ Test 6]\n",
            "\n",
            "Test accuarcy: 73.78\n",
            "Test average loss: 0.007761060801148414\n",
            "Model Saved!\n",
            "\n",
            "[ Train 7]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.7890625\n",
            "Current benign train loss: 0.6169722676277161\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.734375\n",
            "Current benign train loss: 0.7592353224754333\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.8359375\n",
            "Current benign train loss: 0.5461702346801758\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.7734375\n",
            "Current benign train loss: 0.6220995187759399\n",
            "\n",
            "Total benign train accuarcy: 77.644\n",
            "Total benign train loss: 251.5260937511921\n",
            "\n",
            "[ Test 7]\n",
            "\n",
            "Test accuarcy: 74.81\n",
            "Test average loss: 0.007719787701964379\n",
            "Model Saved!\n",
            "\n",
            "[ Train 8]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.84375\n",
            "Current benign train loss: 0.6233235597610474\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.7290031909942627\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.7890625\n",
            "Current benign train loss: 0.538724422454834\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.8046875\n",
            "Current benign train loss: 0.5742416381835938\n",
            "\n",
            "Total benign train accuarcy: 79.74\n",
            "Total benign train loss: 227.28455293178558\n",
            "\n",
            "[ Test 8]\n",
            "\n",
            "Test accuarcy: 75.39\n",
            "Test average loss: 0.007355205878615379\n",
            "Model Saved!\n",
            "\n",
            "[ Train 9]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.8203125\n",
            "Current benign train loss: 0.5413550734519958\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.796875\n",
            "Current benign train loss: 0.584269642829895\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.890625\n",
            "Current benign train loss: 0.3592689335346222\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.796875\n",
            "Current benign train loss: 0.5309380292892456\n",
            "\n",
            "Total benign train accuarcy: 81.762\n",
            "Total benign train loss: 205.27324989438057\n",
            "\n",
            "[ Test 9]\n",
            "\n",
            "Test accuarcy: 73.52\n",
            "Test average loss: 0.008157709112763405\n",
            "Model Saved!\n",
            "\n",
            "[ Train 10]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.8515625\n",
            "Current benign train loss: 0.4361850917339325\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.8671875\n",
            "Current benign train loss: 0.36108893156051636\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.859375\n",
            "Current benign train loss: 0.478036493062973\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.8203125\n",
            "Current benign train loss: 0.5271352529525757\n",
            "\n",
            "Total benign train accuarcy: 83.35\n",
            "Total benign train loss: 187.63990576565266\n",
            "\n",
            "[ Test 10]\n",
            "\n",
            "Test accuarcy: 80.95\n",
            "Test average loss: 0.005564124783873558\n",
            "Model Saved!\n",
            "\n",
            "[ Train 11]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.859375\n",
            "Current benign train loss: 0.4131618142127991\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.8515625\n",
            "Current benign train loss: 0.49862298369407654\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.8203125\n",
            "Current benign train loss: 0.4483790099620819\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.8671875\n",
            "Current benign train loss: 0.39832088351249695\n",
            "\n",
            "Total benign train accuarcy: 84.566\n",
            "Total benign train loss: 173.68664720654488\n",
            "\n",
            "[ Test 11]\n",
            "\n",
            "Test accuarcy: 82.84\n",
            "Test average loss: 0.005052896134555339\n",
            "Model Saved!\n",
            "\n",
            "[ Train 12]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.8671875\n",
            "Current benign train loss: 0.4129473567008972\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.8515625\n",
            "Current benign train loss: 0.32337403297424316\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.890625\n",
            "Current benign train loss: 0.3775624930858612\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.875\n",
            "Current benign train loss: 0.38960692286491394\n",
            "\n",
            "Total benign train accuarcy: 85.678\n",
            "Total benign train loss: 161.87556394934654\n",
            "\n",
            "[ Test 12]\n",
            "\n",
            "Test accuarcy: 83.16\n",
            "Test average loss: 0.005131054517626762\n",
            "Model Saved!\n",
            "\n",
            "[ Train 13]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.8828125\n",
            "Current benign train loss: 0.29219871759414673\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.90625\n",
            "Current benign train loss: 0.3200240731239319\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.875\n",
            "Current benign train loss: 0.33111006021499634\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.828125\n",
            "Current benign train loss: 0.489258736371994\n",
            "\n",
            "Total benign train accuarcy: 86.338\n",
            "Total benign train loss: 153.11037534475327\n",
            "\n",
            "[ Test 13]\n",
            "\n",
            "Test accuarcy: 78.29\n",
            "Test average loss: 0.006684480273723602\n",
            "Model Saved!\n",
            "\n",
            "[ Train 14]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.859375\n",
            "Current benign train loss: 0.3364346921443939\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.7890625\n",
            "Current benign train loss: 0.4381929636001587\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.8984375\n",
            "Current benign train loss: 0.29052209854125977\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.859375\n",
            "Current benign train loss: 0.4324720501899719\n",
            "\n",
            "Total benign train accuarcy: 87.268\n",
            "Total benign train loss: 143.4366693943739\n",
            "\n",
            "[ Test 14]\n",
            "\n",
            "Test accuarcy: 78.84\n",
            "Test average loss: 0.00654457857310772\n",
            "Model Saved!\n",
            "\n",
            "[ Train 15]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.9140625\n",
            "Current benign train loss: 0.28671663999557495\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.8671875\n",
            "Current benign train loss: 0.39178046584129333\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.890625\n",
            "Current benign train loss: 0.36288315057754517\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.8125\n",
            "Current benign train loss: 0.4715522825717926\n",
            "\n",
            "Total benign train accuarcy: 87.848\n",
            "Total benign train loss: 136.39325557649136\n",
            "\n",
            "[ Test 15]\n",
            "\n",
            "Test accuarcy: 85.44\n",
            "Test average loss: 0.004331170234084129\n",
            "Model Saved!\n",
            "\n",
            "[ Train 16]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.8828125\n",
            "Current benign train loss: 0.2886234521865845\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.875\n",
            "Current benign train loss: 0.32002899050712585\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.9140625\n",
            "Current benign train loss: 0.30214640498161316\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.8359375\n",
            "Current benign train loss: 0.44739192724227905\n",
            "\n",
            "Total benign train accuarcy: 88.606\n",
            "Total benign train loss: 129.08189404010773\n",
            "\n",
            "[ Test 16]\n",
            "\n",
            "Test accuarcy: 82.94\n",
            "Test average loss: 0.0051451914936304095\n",
            "Model Saved!\n",
            "\n",
            "[ Train 17]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.9140625\n",
            "Current benign train loss: 0.2762525677680969\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.9609375\n",
            "Current benign train loss: 0.14346608519554138\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.90625\n",
            "Current benign train loss: 0.2901923358440399\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.890625\n",
            "Current benign train loss: 0.32754409313201904\n",
            "\n",
            "Total benign train accuarcy: 89.096\n",
            "Total benign train loss: 121.50289314985275\n",
            "\n",
            "[ Test 17]\n",
            "\n",
            "Test accuarcy: 85.82\n",
            "Test average loss: 0.004238370500504971\n",
            "Model Saved!\n",
            "\n",
            "[ Train 18]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.9140625\n",
            "Current benign train loss: 0.2501436471939087\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.9296875\n",
            "Current benign train loss: 0.2297109067440033\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.8671875\n",
            "Current benign train loss: 0.34050363302230835\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.8671875\n",
            "Current benign train loss: 0.3870360851287842\n",
            "\n",
            "Total benign train accuarcy: 89.584\n",
            "Total benign train loss: 115.64177756756544\n",
            "\n",
            "[ Test 18]\n",
            "\n",
            "Test accuarcy: 84.45\n",
            "Test average loss: 0.004699730101227761\n",
            "Model Saved!\n",
            "\n",
            "[ Train 19]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.921875\n",
            "Current benign train loss: 0.2547181248664856\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.8984375\n",
            "Current benign train loss: 0.3216512203216553\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.8828125\n",
            "Current benign train loss: 0.3266724944114685\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.8984375\n",
            "Current benign train loss: 0.31399574875831604\n",
            "\n",
            "Total benign train accuarcy: 89.9\n",
            "Total benign train loss: 111.98993969708681\n",
            "\n",
            "[ Test 19]\n",
            "\n",
            "Test accuarcy: 84.03\n",
            "Test average loss: 0.005065948018431663\n",
            "Model Saved!\n"
          ]
        }
      ]
    }
  ]
}